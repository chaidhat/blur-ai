{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BlurNet\n",
        "\n"
      ],
      "metadata": {
        "id": "hpY3dexSxNRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the environment"
      ],
      "metadata": {
        "id": "Kdd8beX3TvZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import all relevant modules"
      ],
      "metadata": {
        "id": "_VYL5W_rU3tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "cdpEA10iUfIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install dominate visdom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVYI6MIO4jbe",
        "outputId": "be35d6b0-e8a4-462a-b59b-a7c31271294e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dominate in /usr/local/lib/python3.10/dist-packages (2.9.1)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom) (2.31.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom) (6.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom) (1.16.0)\n",
            "Collecting jsonpatch (from visdom)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom) (1.7.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from visdom) (3.2.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom) (9.4.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2024.2.2)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408194 sha256=1c685514333249ef698f1aef269d3d81a604807639c30b9a4b4c4a8f074775f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built visdom\n",
            "Installing collected packages: jsonpointer, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 visdom-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dataset from Google Drive & Format it"
      ],
      "metadata": {
        "id": "DoMgTsMDV0-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XSLmhFATsyF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5325351d-32bb-450f-87b0-9902c6c9c7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_location = '/content/drive/MyDrive/CS188/cv-project-data.zip'\n",
        "output_location = '/content/data'\n",
        "!echo \"unzipping...\"\n",
        "!rm -rf $output_location\n",
        "!mkdir $output_location\n",
        "!unzip -q $file_location -d $output_location\n",
        "!echo \"done!\""
      ],
      "metadata": {
        "id": "7lahQFM7yI2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f712f3cb-242b-48d2-9396-1d2309132d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzipping...\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the git repo & setup file structure\n",
        "\n"
      ],
      "metadata": {
        "id": "4aS_Az6US96_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf blur-ai\n",
        "!rm -rf tripathy\n",
        "!git clone https://github.com/chaidhat/blur-ai.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUw7Shga4cyf",
        "outputId": "ddc2b12f-dfa7-4f47-affe-b91f1b931afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'blur-ai'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 96 (delta 16), reused 89 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (96/96), 1.17 MiB | 8.91 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute the code"
      ],
      "metadata": {
        "id": "kDXtFoYHS36Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd blur-ai/tripathy ; python3 train.py --dataroot ./datasets --model cycle_gan --dataset_mode unaligned --which_model_netG resnet_9blocks --which_direction AtoB --super_epoch 50 --super_epoch_start 0 --super_mode aligned --super_start 1 --name mygan_70 --no_dropout --niter 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td5d4EOTKnRE",
        "outputId": "9cead7c8-c3df-4ff2-c3b9-7e4874589742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 1\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: False\n",
            "dataroot: ./datasets\n",
            "dataset_mode: unaligned\n",
            "display_freq: 100\n",
            "display_id: 1\n",
            "display_port: 8097\n",
            "display_single_pane_ncols: 0\n",
            "display_winsize: 256\n",
            "epoch_count: 1\n",
            "fineSize: 256\n",
            "gpu_ids: [0]\n",
            "identity: 0.5\n",
            "init_type: normal\n",
            "input_nc: 3\n",
            "isTrain: True\n",
            "lambda_A: 10.0\n",
            "lambda_B: 10.0\n",
            "loadSize: 286\n",
            "lr: 0.0002\n",
            "lr_decay_iters: 50\n",
            "lr_policy: lambda\n",
            "max_dataset_size: inf\n",
            "model: cycle_gan\n",
            "nThreads: 2\n",
            "n_layers_D: 3\n",
            "name: mygan_70\n",
            "ndf: 64\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "no_dropout: True\n",
            "no_flip: False\n",
            "no_html: False\n",
            "no_lsgan: False\n",
            "norm: instance\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 50\n",
            "print_freq: 100\n",
            "resize_or_crop: resize_and_crop\n",
            "save_epoch_freq: 5\n",
            "save_latest_freq: 5000\n",
            "serial_batches: False\n",
            "super_epoch: 50\n",
            "super_epoch_start: 0\n",
            "super_mode: aligned\n",
            "super_start: 1\n",
            "update_html_freq: 1000\n",
            "which_direction: AtoB\n",
            "which_epoch: latest\n",
            "which_model_netD: basic\n",
            "which_model_netG: resnet_9blocks\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [UnalignedDataset] was created\n",
            "CustomDatasetDataLoader_super\n",
            "dataset [AlignedDataset] was created\n",
            "#training images = 2\n",
            "cycle_gan\n",
            "initialization method [normal]\n",
            "/content/blur-ai/tripathy/models/networks.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  init.normal(m.weight.data, 0.0, 0.02)\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "LSGAN/////////////////////////////////////////////////////////////\n",
            "---------- Networks initialized -------------\n",
            "ResnetGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (11): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (12): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (13): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (14): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (15): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (27): Tanh()\n",
            "  )\n",
            ")\n",
            "Total number of parameters: 11378179\n",
            "ResnetGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (11): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (12): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (13): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (14): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (15): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (27): Tanh()\n",
            "  )\n",
            ")\n",
            "Total number of parameters: 11378179\n",
            "NLayerDiscriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "Total number of parameters: 2764737\n",
            "NLayerDiscriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "Total number of parameters: 2764737\n",
            "-----------------------------------------------\n",
            "model [CycleGANModel] was created\n",
            "/content/blur-ai/tripathy/models/networks.py:185: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
            "End of epoch 1 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 2 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 3 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 4 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 5, iters 10\n",
            "End of epoch 5 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 6 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 7 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 8 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 9 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 10, iters 20\n",
            "End of epoch 10 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 11 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 12 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 13 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 14 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 15, iters 30\n",
            "End of epoch 15 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 16 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 17 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 18 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 19 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 20, iters 40\n",
            "End of epoch 20 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 21 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 22 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 23 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 24 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 25, iters 50\n",
            "End of epoch 25 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 26 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 27 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 28 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 29 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 30, iters 60\n",
            "End of epoch 30 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 31 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 32 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 33 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 34 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 35, iters 70\n",
            "End of epoch 35 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 36 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 37 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 38 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 39 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 40, iters 80\n",
            "End of epoch 40 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 41 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 42 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 43 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 44 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 45, iters 90\n",
            "End of epoch 45 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 46 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 47 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 48 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 49 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 50, iters 100\n",
            "End of epoch 50 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 51 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 52 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 53 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 54 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 55, iters 110\n",
            "End of epoch 55 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 56 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 57 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 58 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 59 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 60, iters 120\n",
            "End of epoch 60 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 61 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 62 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 63 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 64 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 65, iters 130\n",
            "End of epoch 65 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 66 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 67 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 68 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 69 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 70, iters 140\n",
            "End of epoch 70 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 71 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 72 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 73 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 74 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 75, iters 150\n",
            "End of epoch 75 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 76 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 77 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 78 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 79 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 80, iters 160\n",
            "End of epoch 80 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 81 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 82 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 83 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 84 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 85, iters 170\n",
            "End of epoch 85 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 86 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 87 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 88 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 89 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 90, iters 180\n",
            "End of epoch 90 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 91 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 92 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 93 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 94 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "saving the model at the end of epoch 95, iters 190\n",
            "End of epoch 95 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 96 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 97 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 98 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0002000\n",
            "End of epoch 99 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001980\n",
            "saving the model at the end of epoch 100, iters 200\n",
            "End of epoch 100 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001960\n",
            "End of epoch 101 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001941\n",
            "End of epoch 102 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001921\n",
            "End of epoch 103 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001901\n",
            "End of epoch 104 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001881\n",
            "saving the model at the end of epoch 105, iters 210\n",
            "End of epoch 105 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001861\n",
            "End of epoch 106 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001842\n",
            "End of epoch 107 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001822\n",
            "End of epoch 108 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001802\n",
            "End of epoch 109 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001782\n",
            "saving the model at the end of epoch 110, iters 220\n",
            "End of epoch 110 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001762\n",
            "End of epoch 111 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001743\n",
            "End of epoch 112 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001723\n",
            "End of epoch 113 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001703\n",
            "End of epoch 114 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001683\n",
            "saving the model at the end of epoch 115, iters 230\n",
            "End of epoch 115 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001663\n",
            "End of epoch 116 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001644\n",
            "End of epoch 117 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001624\n",
            "End of epoch 118 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001604\n",
            "End of epoch 119 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001584\n",
            "saving the model at the end of epoch 120, iters 240\n",
            "End of epoch 120 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001564\n",
            "End of epoch 121 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001545\n",
            "End of epoch 122 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001525\n",
            "End of epoch 123 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001505\n",
            "End of epoch 124 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001485\n",
            "saving the model at the end of epoch 125, iters 250\n",
            "End of epoch 125 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001465\n",
            "End of epoch 126 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001446\n",
            "End of epoch 127 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001426\n",
            "End of epoch 128 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001406\n",
            "End of epoch 129 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001386\n",
            "saving the model at the end of epoch 130, iters 260\n",
            "End of epoch 130 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001366\n",
            "End of epoch 131 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001347\n",
            "End of epoch 132 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001327\n",
            "End of epoch 133 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001307\n",
            "End of epoch 134 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001287\n",
            "saving the model at the end of epoch 135, iters 270\n",
            "End of epoch 135 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001267\n",
            "End of epoch 136 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001248\n",
            "End of epoch 137 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001228\n",
            "End of epoch 138 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001208\n",
            "End of epoch 139 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001188\n",
            "saving the model at the end of epoch 140, iters 280\n",
            "End of epoch 140 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001168\n",
            "End of epoch 141 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001149\n",
            "End of epoch 142 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001129\n",
            "End of epoch 143 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001109\n",
            "End of epoch 144 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001089\n",
            "saving the model at the end of epoch 145, iters 290\n",
            "End of epoch 145 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0001069\n",
            "End of epoch 146 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001050\n",
            "End of epoch 147 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001030\n",
            "End of epoch 148 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0001010\n",
            "End of epoch 149 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000990\n",
            "saving the model at the end of epoch 150, iters 300\n",
            "End of epoch 150 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000970\n",
            "End of epoch 151 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000950\n",
            "End of epoch 152 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000931\n",
            "End of epoch 153 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000911\n",
            "End of epoch 154 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000891\n",
            "saving the model at the end of epoch 155, iters 310\n",
            "End of epoch 155 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000871\n",
            "End of epoch 156 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000851\n",
            "End of epoch 157 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000832\n",
            "End of epoch 158 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000812\n",
            "End of epoch 159 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000792\n",
            "saving the model at the end of epoch 160, iters 320\n",
            "End of epoch 160 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000772\n",
            "End of epoch 161 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000752\n",
            "End of epoch 162 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000733\n",
            "End of epoch 163 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000713\n",
            "End of epoch 164 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000693\n",
            "saving the model at the end of epoch 165, iters 330\n",
            "End of epoch 165 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000673\n",
            "End of epoch 166 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000653\n",
            "End of epoch 167 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000634\n",
            "End of epoch 168 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000614\n",
            "End of epoch 169 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000594\n",
            "saving the model at the end of epoch 170, iters 340\n",
            "End of epoch 170 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000574\n",
            "End of epoch 171 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000554\n",
            "End of epoch 172 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000535\n",
            "End of epoch 173 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000515\n",
            "End of epoch 174 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000495\n",
            "saving the model at the end of epoch 175, iters 350\n",
            "End of epoch 175 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000475\n",
            "End of epoch 176 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000455\n",
            "End of epoch 177 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000436\n",
            "End of epoch 178 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000416\n",
            "End of epoch 179 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000396\n",
            "saving the model at the end of epoch 180, iters 360\n",
            "End of epoch 180 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000376\n",
            "End of epoch 181 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000356\n",
            "End of epoch 182 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000337\n",
            "End of epoch 183 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000317\n",
            "End of epoch 184 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000297\n",
            "saving the model at the end of epoch 185, iters 370\n",
            "End of epoch 185 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000277\n",
            "End of epoch 186 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000257\n",
            "End of epoch 187 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000238\n",
            "End of epoch 188 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000218\n",
            "End of epoch 189 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000198\n",
            "saving the model at the end of epoch 190, iters 380\n",
            "End of epoch 190 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000178\n",
            "End of epoch 191 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000158\n",
            "End of epoch 192 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000139\n",
            "End of epoch 193 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000119\n",
            "End of epoch 194 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000099\n",
            "saving the model at the end of epoch 195, iters 390\n",
            "End of epoch 195 / 200 \t Time Taken: 2 sec\n",
            "learning rate = 0.0000079\n",
            "End of epoch 196 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000059\n",
            "End of epoch 197 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000040\n",
            "End of epoch 198 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000020\n",
            "End of epoch 199 / 200 \t Time Taken: 1 sec\n",
            "learning rate = 0.0000000\n",
            "saving the model at the end of epoch 200, iters 400\n",
            "End of epoch 200 / 200 \t Time Taken: 2 sec\n",
            "learning rate = -0.0000020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd blur-ai/tripathy ; python test.py --dataroot ./datasets --model cycle_gan --dataset_mode unaligned --which_model_netG resnet_9blocks --which_direction AtoB --name mygan_70 --how_many 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF6bmdOQePVO",
        "outputId": "fa934c6d-03d5-401a-f986-324432b621c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "aspect_ratio: 1.0\n",
            "batchSize: 1\n",
            "checkpoints_dir: ./checkpoints\n",
            "dataroot: ./datasets\n",
            "dataset_mode: unaligned\n",
            "display_id: 1\n",
            "display_port: 8097\n",
            "display_winsize: 256\n",
            "fineSize: 256\n",
            "gpu_ids: [0]\n",
            "how_many: 100\n",
            "init_type: normal\n",
            "input_nc: 3\n",
            "isTrain: False\n",
            "loadSize: 286\n",
            "max_dataset_size: inf\n",
            "model: cycle_gan\n",
            "nThreads: 2\n",
            "n_layers_D: 3\n",
            "name: mygan_70\n",
            "ndf: 64\n",
            "ngf: 64\n",
            "no_dropout: False\n",
            "no_flip: False\n",
            "norm: instance\n",
            "ntest: inf\n",
            "output_nc: 3\n",
            "phase: test\n",
            "resize_or_crop: resize_and_crop\n",
            "results_dir: ./results/\n",
            "serial_batches: False\n",
            "which_direction: AtoB\n",
            "which_epoch: latest\n",
            "which_model_netD: basic\n",
            "which_model_netG: resnet_9blocks\n",
            "-------------- End ----------------\n",
            "CustomDatasetDataLoader\n",
            "dataset [UnalignedDataset] was created\n",
            "cycle_gan\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/blur-ai/tripathy/test.py\", line 17, in <module>\n",
            "    model = create_model(opt)\n",
            "  File \"/content/blur-ai/tripathy/models/models.py\", line 19, in create_model\n",
            "    model.initialize(opt)\n",
            "  File \"/content/blur-ai/tripathy/models/cycle_gan_model.py\", line 49, in initialize\n",
            "    self.load_network(self.netG_A, 'G_A', which_epoch)\n",
            "  File \"/content/blur-ai/tripathy/models/base_model.py\", line 53, in load_network\n",
            "    network.load_state_dict(torch.load(save_path))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for ResnetGenerator:\n",
            "\tMissing key(s) in state_dict: \"model.10.conv_block.6.weight\", \"model.10.conv_block.6.bias\", \"model.11.conv_block.6.weight\", \"model.11.conv_block.6.bias\", \"model.12.conv_block.6.weight\", \"model.12.conv_block.6.bias\", \"model.13.conv_block.6.weight\", \"model.13.conv_block.6.bias\", \"model.14.conv_block.6.weight\", \"model.14.conv_block.6.bias\", \"model.15.conv_block.6.weight\", \"model.15.conv_block.6.bias\", \"model.16.conv_block.6.weight\", \"model.16.conv_block.6.bias\", \"model.17.conv_block.6.weight\", \"model.17.conv_block.6.bias\", \"model.18.conv_block.6.weight\", \"model.18.conv_block.6.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"model.10.conv_block.5.weight\", \"model.10.conv_block.5.bias\", \"model.11.conv_block.5.weight\", \"model.11.conv_block.5.bias\", \"model.12.conv_block.5.weight\", \"model.12.conv_block.5.bias\", \"model.13.conv_block.5.weight\", \"model.13.conv_block.5.bias\", \"model.14.conv_block.5.weight\", \"model.14.conv_block.5.bias\", \"model.15.conv_block.5.weight\", \"model.15.conv_block.5.bias\", \"model.16.conv_block.5.weight\", \"model.16.conv_block.5.bias\", \"model.17.conv_block.5.weight\", \"model.17.conv_block.5.bias\", \"model.18.conv_block.5.weight\", \"model.18.conv_block.5.bias\". \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IGNORE BELOW"
      ],
      "metadata": {
        "id": "R6VsLvawTi-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed code from the Assignments to reset seed"
      ],
      "metadata": {
        "id": "VK0Ve_saUZlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import os\n",
        "    import random\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "seed_everything(0)"
      ],
      "metadata": {
        "id": "ZVZnzZC5UM-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our model"
      ],
      "metadata": {
        "id": "XNhm6PKQTssa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luMe4_QnTMAU"
      },
      "outputs": [],
      "source": [
        "# Define your model.\n",
        "class BlurNet(nn.Module):\n",
        "\n",
        "    def __init__(self, mode='finetune', num_classes=100):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        use the resnet18 model from torchvision models. Remember to set pretrained as true\n",
        "\n",
        "        mode has two options:\n",
        "        1) linear: For this model, we want to freeze resnet18 features, then train a linear\n",
        "            classifier which takes the features before resnet.fc (we do not want\n",
        "            the pretrained resnet.fc for 1000 classes). And then write our own FC layer: which takes in the features and\n",
        "            output scores of size 100 (because we have 100 categories).\n",
        "            Because we want to freeze resnet18 features, we have to iterate through parameters()\n",
        "            of our model, and manually set some parameters to requires_grad = False\n",
        "            Or use other methods to freeze the features\n",
        "        2) finetune: Same as 1), except that we we do not need to freeze the features and\n",
        "           can finetune on the pretrained resnet model.\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        self.resnet = models.resnext50_32x4d(pretrained=True)\n",
        "        if mode == \"linear\":\n",
        "          for name, param in self.resnet.named_parameters():\n",
        "            if not name.startswith('fc'):\n",
        "                param.requires_grad = False\n",
        "        elif mode != \"finetune\":\n",
        "          print(\"unrecognized mode!\")\n",
        "\n",
        "        self.resnet.fc = nn.Linear(2048, num_classes, device=device)\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ################# Your Implementations #################################\n",
        "        x = self.resnet(x)\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n",
        "\n",
        "    def to(self, device):\n",
        "        return self.resnet.to(device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "VqcMHOthTsMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build train/val/test datasets and data loaders using the modules we built before.\n",
        "import os\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class MiniPlacesTest(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, transform):\n",
        "        \"\"\"\n",
        "        Initialize the MiniPlaces test dataset with the root directory for the images,\n",
        "        an optional data transformation,\n",
        "\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Root directory for the MiniPlaces images.\n",
        "            transform (callable, optional): Optional data transformation to apply to the images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.ids = []\n",
        "\n",
        "        # You should read the test images from '${root_dir}/images/test' dir and save their filepaths and ids\n",
        "        # For instance: the id of image 'test/00000001.jpg' should be '00000001' in string\n",
        "        ################# Your Implementations #################################\n",
        "        self.labels = []\n",
        "        self.length = 0\n",
        "        for filename in os.scandir(f'{root_dir}/images/test'):\n",
        "          if filename.is_file():\n",
        "            self.length += 1\n",
        "            self.filenames.append(filename.name)\n",
        "            self.ids.append(filename.name.split(\"/\")[-1].split(\".\")[0])\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of images in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of images in the dataset.\n",
        "        \"\"\"\n",
        "        dataset_len = 0\n",
        "        ################# Your Implementations #################################\n",
        "        # Return the number of images in the dataset\n",
        "        dataset_len = self.length\n",
        "        ################# End of your Implementations ##########################\n",
        "        return dataset_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return a single image and its corresponding id when given an index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the image and its id.\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # Load and preprocess image using self.root_dir,\n",
        "        # self.filenames[idx], and self.transform (if specified)\n",
        "\n",
        "        pretransformed_image = Image.open(f'{root_dir}/data/images/test/{self.filenames[idx]}')\n",
        "        image = data_transform(pretransformed_image)\n",
        "        id = self.ids[idx]\n",
        "        ################# End of your Implementations ##########################\n",
        "        return image, id\n",
        "\n",
        "\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "# Create MiniPlaces test dataset object\n",
        "miniplaces_test = MiniPlacesTest(data_root, transform=data_transform)\n",
        "\n",
        "# Create DataLoader for test set\n",
        "test_loader = DataLoader(miniplaces_test,\n",
        "                         batch_size=batch_size,\n",
        "                         num_workers=num_workers,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "Syr3kx3zTxAL",
        "outputId": "94f90cdb-54d8-42e2-abc5-c9f5dd788b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'root_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-79f7c23f2197>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mdata_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;31m# Create MiniPlaces test dataset object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mminiplaces_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniPlacesTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'root_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Citation\n",
        "\n",
        "@article{tripathy+kannala+rahtu,\n",
        "  title={Learning image-to-image translation using paired and unpaired training samples},\n",
        "  author={Tripathy, Soumya and Kannala, Juho and Rahtu, Esa},\n",
        "  journal={arXiv preprint arXiv:1805.03189},\n",
        "  year={2018}\n",
        "}"
      ],
      "metadata": {
        "id": "lTCQWB0zcwBm"
      }
    }
  ]
}